
print('Chapter 6, Exercises, Conceptual parts')
print('Excercise 1: (a). The best subset has the smallest training RSS, because it traversal all the possible models.')
print('Excercise 1: (b). The best subset has the smallest test RSS, because it considers more models than the other 2 methods.')
print('Excercise 1: (c) i. True. It\'s obviously.')
print('Excercise 1: (c) ii. True. It\'s obviously also.')
print('Excercise 1: (c) iii. False. It can\'t gurantee pick predictor in the same order.')
print('Excercise 1: (c) iv. False. Same as iii.')
print('Excercise 1: (c) v. False. Same as iii.')
print('Excercise 2: (a) iii is True.')
print('Excercise 2: (b) iii is True.')
print('Excercise 2: (c) iv is True.')
print('Excercise 3: (a) ii is True. It can suppose s is 0, the training RSS will from large to small if s increase. Finally, it will same as least square result.')
print('Excercise 3: (b) ii is True.')
print('Excercise 3: (c) iii is True.')
print('Excercise 3: (d) ii is True.')
print('Excercise 3: (e) v is True.')
print('Excercise 4: (a) iii is True.')
print('Excercise 4: (b) iii is True.')
print('Excercise 4: (c) iv is True.')
print('Excercise 4: (d) iii is True.')
print('Excercise 4: (e) v is True.')
print('Excercise 5: (a) expand ridge regression formula.')
print('Excercise 5: (b) take derivative of beta1 and beta2 seperatedly on optimization formula.')
print('Excercise 5: (c) same as Quation A.')
print('Excercise 5: (d) I don\t know the answers.')

print('Excercise 6: ignore.')
print('Excercise 7: ignore')

